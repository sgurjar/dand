<h1>P5: Identify Fraud from Enron Email</h1>

<h2>Objective</h2>

<p>Objecive of this project to find Person of Interest (POI) using Enron dataset.
POI is someone who is involved in the fraud.</p>

<h2>Dataset</h2>

<p>Enron dataset contains 145 records, each record has 20 features. Label <code>poi</code> is
boolean value that is <code>True</code> for Person of Interest. There are 18 POI in the dataset.
This is binary classification problem when we need to predict True or False for a
given input. Following is one exmaple of a record in dataset.</p>

<pre><code>{ 'LEFF DANIEL P':
  { 'salary': 273746,
    'to_messages': 2822,
    'deferral_payments': 'NaN',
    'total_payments': 2664228,
    'exercised_stock_options': 'NaN',
    'bonus': 1000000,
    'restricted_stock': 360528,
    'shared_receipt_with_poi': 2672,
    'restricted_stock_deferred': 'NaN',
    'total_stock_value': 360528,
    'expenses': 'NaN',
    'loan_advances': 'NaN',
    'from_messages': 63,
    'other': 3083,
    'from_this_person_to_poi': 14,
    'poi': False,
    'director_fees': 'NaN',
    'deferred_income': 'NaN',
    'long_term_incentive': 1387399,
    'email_address': 'dan.leff@enron.com',
    'from_poi_to_this_person': 67
  }
}
</code></pre>

<h2>Visualize Dataset</h2>

<p>I tried to visualize dataset, however it didnt provide me sufficient information, so
it wasnt very useful. Following are the plots I did:</p>

<p><img src="_img/plot1.png" alt="Figure 1" title="" />
<img src="_img/plot2.png" alt="Figure 2" title="" />
<img src="_img/plot3.png" alt="Figure 3" title="" /></p>

<h2>Outlier</h2>

<ul>
<li><code>find_outliers</code> method is used to find outliers, it print top 10 employees by salary,
bonus, total<em>payments and total</em>stock_value.</li>
<li>Data with key <code>TOTAL</code> has very high number for all 4 features. Also by key name <code>TOTAL</code>
is appears this sum of all records, this is definitely outlier and we remove this.</li>
<li>There are other employees who has very high salary, bonus, total<em>payments and total</em>stock_value.
We compare it with median of that feature. However, this should be real data (not outlier) and
important aspect of detecting POI.</li>
</ul>

<h2>Feature Selection</h2>

<ul>
<li>I have added all financial and email features and later on used PCA to reduce number of
features to 5.</li>
<li>Created two new features, <code>fraction_to_poi</code> and  <code>fraction_from_poi</code>. <code>fraction_to_poi</code> is
fraction of email sent by an employee to POI. <code>fraction_from_poi</code> is number of emails an
employee received from POI.</li>
<li>Later on while tunning KNN and DTC algorithms, it appears newly created features are reducing
precision and recall, so I have removed these features.</li>
</ul>

<h2>Compare classfication algorithms</h2>

<p>We comapred following classification algorithms.</p>

<ul>
<li>RandomForestClassifier</li>
<li>GradientBoostingClassifier</li>
<li>LogisticRegression</li>
<li>GaussianNB</li>
<li>SVM</li>
<li>KNeighborsClassifier</li>
<li>DecisionTreeClassifier</li>
<li>AdaBoostClassifier</li>
</ul>

<p>For each classfier we create a pipeline, with PCA and classifier steps, PCA
reduces the number of features to 5. Following is result of running <code>test_classifier</code>
on each classifier.</p>

<pre><code>### RandomForestClassifier
Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('RandomForestClassifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes...stimators=10, n_jobs=1, oob_score=False, random_state=42,
            verbose=0, warm_start=False))])
    Accuracy: 0.85713   Precision: 0.40654  Recall: 0.15550 F1: 0.22495 F2: 0.17741
    Total predictions: 15000    True positives:  311    False positives:  454   False negatives: 1689   True negatives: 12546

### GradientBoostingClassifier
Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('GradientBoostingClassifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
        ...rs=100, presort='auto', random_state=42,
              subsample=1.0, verbose=0, warm_start=False))])
    Accuracy: 0.84320   Precision: 0.36944  Recall: 0.24900 F1: 0.29749 F2: 0.26637
    Total predictions: 15000    True positives:  498    False positives:  850   False negatives: 1502   True negatives: 12150

### LogisticRegression
Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('LogisticRegression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,
          verbose=0, warm_start=False))])
    Accuracy: 0.83680   Precision: 0.31240  Recall: 0.18650 F1: 0.23356 F2: 0.20285
    Total predictions: 15000    True positives:  373    False positives:  821   False negatives: 1627   True negatives: 12179

### GaussianNB
Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('GaussianNB', GaussianNB(priors=None))])
    Accuracy: 0.86393   Precision: 0.47931  Recall: 0.23750 F1: 0.31762 F2: 0.26415
    Total predictions: 15000    True positives:  475    False positives:  516   False negatives: 1525   True negatives: 12484

### SVM
Got a divide by zero when trying out: Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('SVM', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
  max_iter=-1, probability=False, random_state=42, shrinking=True,
  tol=0.001, verbose=False))])
Precision or recall may be undefined due to a lack of true positive predicitons.

### KNeighborsClassifier
Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('KNeighborsClassifier', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform'))])
    Accuracy: 0.88767   Precision: 0.74119  Recall: 0.24200 F1: 0.36487 F2: 0.27967
    Total predictions: 15000    True positives:  484    False positives:  169   False negatives: 1516   True negatives: 12831

### DecisionTreeClassifier
Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('DecisionTreeClassifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_split=1e-07, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            presort=False, random_state=42, splitter='best'))])
    Accuracy: 0.81340   Precision: 0.29628  Recall: 0.29050 F1: 0.29336 F2: 0.29164
    Total predictions: 15000    True positives:  581    False positives: 1380   False negatives: 1419   True negatives: 11620

### AdaBoostClassifier
Pipeline(steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,
  svd_solver='auto', tol=0.0, whiten=False)), ('AdaBoostClassifier', AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,
          learning_rate=1.0, n_estimators=50, random_state=42))])
    Accuracy: 0.81527   Precision: 0.24919  Recall: 0.19150 F1: 0.21657 F2: 0.20080
    Total predictions: 15000    True positives:  383    False positives: 1154   False negatives: 1617   True negatives: 11846
</code></pre>

<ul>
<li>Here, DecisionTreeClassifier is close to <code>.3</code> Precision and Recall</li>
<li>KNeighborsClassifier has very high Precision <code>0.74119</code> and Recall close to <code>.3</code>, <code>0.24200</code>.</li>
<li>We will try to tune these 2 classifiers.</li>
</ul>
